{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import flammkuchen as fl\n",
    "import napari\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "from split_dataset import SplitDataset\n",
    "import json\n",
    "import tifffile\n",
    "\n",
    "from bg_atlasapi.bg_atlas import BrainGlobeAtlas\n",
    "from bg_space import AnatomicalSpace\n",
    "\n",
    "from quickdisplay import *\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(r\"\\\\portulab.synology.me\\data\\Hagar and Ot\\E0040\\v10\\LS\")\n",
    "path_list = list((data_dir).glob('*f[0-9]*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load reference stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify fish to use as reference, and import stack and metadata to define image resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MPI reference stack\n",
    "# mpi_ref = BrainGlobeAtlas('mpin_zfish_1um')\n",
    "# ref = mpi_ref.additional_references['H2BGCaMP']\n",
    "ref_path = Path(r'\\\\portulab.synology.me\\data\\Hagar and Ot\\good h2b reference from mapzebrain')\n",
    "ref = tifffile.imread(ref_path / 'T_AVG_HuCH2BGCaMP2-tg_ch0.tif') \n",
    "\n",
    "#Store resolution\n",
    "#mpi_ref.metadata['resolution']\n",
    "ref_res = [1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load moving stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify fish to register to the reference stack. Directory will be used as output for the resampled images and initial transformation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify fish to register\n",
    "fish_path = path_list[0]\n",
    "print('Morphing: ', fish_path)\n",
    "\n",
    "#Load functional dataset to align\n",
    "mov = tifffile.imread(fish_path / 'anatomy_suite2p.tif') #suite2p\n",
    "\n",
    "#Retrieve resolution\n",
    "with open(next((fish_path).glob('*metadata*')), \"r\") as f:\n",
    "        ref_metadata = json.load(f)\n",
    "\n",
    "ls_config = ref_metadata['imaging']['microscope_config']['lightsheet']['scanning']\n",
    "z_tot_span = ls_config[\"z\"][\"piezo_max\"] - ls_config[\"z\"][\"piezo_min\"]\n",
    "n_planes = ls_config[\"triggering\"][\"n_planes\"]\n",
    "\n",
    "z_res = z_tot_span / n_planes\n",
    "mov_res = [z_res, 0.6, 0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to make sure both stacks are in the same common space. When regsitering reference brains you may want to downsample their resolutions a bit as mapping between spaces later might be very time-consuming, and not all the resolution information is needed. However, when working with functional stacks, you probably don't want to do any downsampling (especially in z) to preserve as much information as possible.\n",
    "\n",
    "The most important part is to make sure you define the correct `origin` when creating the `AnatomicaSpace` objects. This will change between setups or depending on the orientation of your stacks. You can check the documentation of `bg-space` [here](https://github.com/brainglobe/bg-space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_as = AnatomicalSpace('ial', resolution=ref_res, shape=ref.shape) #MPI ref\n",
    "mov_as = AnatomicalSpace('ipl', resolution=mov_res, shape=mov.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a morphing space, and map both of our stacks into it. In this case, despite you could keep the `origin` of your moprhing space to be the same as your functional stacks, I suggest sticking to the `'rai'` one if you plan on doing the morphing with ANTsPy. ANTsPy works with [ITK coordinate space](https://www.slicer.org/wiki/Coordinate_systems), so sticking to this option might make your life easier on the long run.\n",
    "\n",
    "I also suggest upsampling the resolution along the z dimension. This will generate new planes by interpolating the info between adjacent planes. Might not make a lot of sense since it will make everything more computationally-intensive, but in my hands, ANTsPy registration always worked better when passing to it stacks upsampled across the z dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define morphing space...\n",
    "morphing_as = AnatomicalSpace('rai', resolution=(1,1,1))\n",
    "\n",
    "#... and transform references to morphing space\n",
    "ref_mapped = ref_as.map_stack_to(morphing_as, ref)\n",
    "mov_mapped = mov_as.map_stack_to(morphing_as, mov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load the coordinates for our ROIs and map them to the same morphing space. This is essential if you want the final registrations to be applied properly to your ROI coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ROI coordinates...\n",
    "mov_roi_coords = fl.load(fish_path/'data_from_suite2p_cells.h5')['coords'] #suite2p\n",
    "\n",
    "#...and map them as well to the morphing space\n",
    "mov_roi_coords_mapped = mov_as.map_points_to(morphing_as, mov_roi_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety check\n",
    "We can check that both stacks and points are in the same space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_side_to_side,\n",
    "        stack1=fixed(ref_mapped),\n",
    "        stack2=fixed(mov_mapped),\n",
    "        depth = widgets.IntSlider(min=0, max=100, step=5, continuous_update=False),\n",
    "        dim=fixed(2), \n",
    "        stack1_title=fixed('Ref fish'),\n",
    "        stack2_title=fixed('Functional stack'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(np.nanmean(ref_mapped, 2), cmap='gray_r', origin='lower') \n",
    "axes[0].set_title('Ref ROIs')\n",
    "\n",
    "axes[1].imshow(np.nanmean(mov_mapped, 2), cmap='gray_r', origin='lower') \n",
    "axes[1].scatter(mov_roi_coords_mapped[:, 1], mov_roi_coords_mapped[:, 0], c='red', alpha=.0075)\n",
    "axes[1].set_title('Mov ROIs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_dir = fish_path / 'registration' / 'to_h2b_baier_ref' / 'antspy'\n",
    "\n",
    "if not os.path.isdir(registration_dir):\n",
    "    os.makedirs(registration_dir)\n",
    "    \n",
    "#Store mapped stacks\n",
    "fl.save(registration_dir / \"ref_mapped.h5\", ref_mapped)\n",
    "fl.save(registration_dir / \"mov_mapped.h5\", mov_mapped)\n",
    "\n",
    "#Store mapped ROI coordinates\n",
    "# fl.save(registration_dir / \"ref_roi_coords_mapped.h5\", ref_roi_coords_mapped)\n",
    "# fl.save(registration_dir / \"mov_roi_coords_mapped.h5\", mov_roi_coords_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual affine transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by finding an initial [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) based on manually-defined points. This is useful in order to get an initial rough estimate of the necessary transformation matrix needed to merge our datasets.\n",
    "\n",
    "To do so, we will open the reference and the dataset stacks in napari, and [draw some points](https://napari.org/tutorials/fundamentals/points.html) in equivalent regions of the anatomies. Start always with identifiable regions/structures in the anatomy with the lowest resolution, and find these same spots in the reference anatomy. Some good areas to try:\n",
    "- Extreme brain regions (top-most, rostral telencephalon...)\n",
    "- Well-defined structures (inferior olive, IPN, Mauthner cells...)\n",
    "- Centers or borders of larger structures (Habenulae, Optic tectums)\n",
    "- ...\n",
    "\n",
    "Make sure you add points in the two different stacks in the same order, or the resulting transformation matrix will make no sense. <br>\n",
    "You don't need to go crazy adding infinite points: ~6-8 points scattered around the brain should be more than enough to get a decent initial estimate. **UPDATE** I realized I prefer to go crazy adding infinite points. Whatever it takes for my initial guess to decently register brain areas within the brain as well as the general contour of the brain at different depths. Not sure How achievable it is to do both things simultaneously with only this tool, but honnestly it will save a bunch of time trying to optimize parameters in ANTsPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_stack, mov_stack = ref_mapped, mov_mapped\n",
    "\n",
    "#Open stacks in napari\n",
    "functional_viewer = napari.view_image(mov_stack)\n",
    "reference_viewer = napari.view_image(ref_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we retrieve the points we just defined...\n",
    "functional_points = functional_viewer.layers[1].data\n",
    "reference_points = reference_viewer.layers[1].data\n",
    "\n",
    "X = np.pad(reference_points, ((0,0), (0,1)), mode=\"constant\", constant_values=1)\n",
    "Y = functional_points\n",
    "\n",
    "#...and we use some math magic to perform an initial fit \n",
    "#(this line performs some sort of lineal regression to find a transform matrix between our two arrays of points)\n",
    "transform_mat =  (np.linalg.pinv(X.T @ X) @ X.T @ Y).T\n",
    "\n",
    "#Apply transform\n",
    "transformed = map_affine(mov_stack, transform_mat, ref_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Or check preexisting transform###\n",
    "transform_mat = fl.load(registration_dir / \"initial_transform_mapped.h5\") #Path to initial transform\n",
    "transformed = map_affine(mov_mapped, transform_mat, ref_mapped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize initial approximation\n",
    "viewer = napari.view_image(ref_mapped, colormap=\"green\")\n",
    "viewer.add_image(transformed, colormap=\"magenta\", blending=\"additive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the initial transform if happy\n",
    "fl.save(registration_dir / \"initial_transform_mapped.h5\", transform_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANTsPy RegistrationÂ¶\n",
    "Now you should proceed onto the ANTsPy registration. To do so, open the next notebook in the repository with JupyterLab in your linux terminal. Then load the files you have generated in this notebook to your linux virtual subsystem, and follow the instructions of the next notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
